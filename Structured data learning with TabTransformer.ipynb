{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6fe8af9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d1f85",
   "metadata": {},
   "source": [
    "# Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c78ca121",
   "metadata": {},
   "outputs": [],
   "source": [
    "#our features\n",
    "CSV_HEADER = [\n",
    "    \"age\",\n",
    "    \"workclass\",\n",
    "    \"fnlwgt\",\n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"gender\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"income_bracket\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d3d3ddbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the train data\n",
    "train_data_url = (\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "139e0abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (32561, 15)\n",
      "Test dataset shape: (16282, 15)\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(train_data_url, header=None, names=CSV_HEADER)\n",
    "#import test data\n",
    "test_data_url = (\"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test\")\n",
    "test_data = pd.read_csv(test_data_url, header=None, names=CSV_HEADER)\n",
    "#display the shape of this file\n",
    "print(f\"Train dataset shape: {train_data.shape}\")\n",
    "print(f\"Test dataset shape: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13889c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the first record (because it is not a valid data example) and a trailing 'dot' in the class labels\n",
    "test_data = test_data[1:]\n",
    "test_data.income_bracket = test_data.income_bracket.apply(\n",
    "    lambda value: value.replace(\".\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a78ebaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we store the training and test data in separate CSV files\n",
    "train_data_file = \"train_data.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=False)\n",
    "test_data.to_csv(test_data_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8800e41c",
   "metadata": {},
   "source": [
    "# Define dataset metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b60fc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"age\",\n",
    "    \"education_num\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "]\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"workclass\": sorted(list(train_data[\"workclass\"].unique())),\n",
    "    \"education\": sorted(list(train_data[\"education\"].unique())),\n",
    "    \"marital_status\": sorted(list(train_data[\"marital_status\"].unique())),\n",
    "    \"occupation\": sorted(list(train_data[\"occupation\"].unique())),\n",
    "    \"relationship\": sorted(list(train_data[\"relationship\"].unique())),\n",
    "    \"race\": sorted(list(train_data[\"race\"].unique())),\n",
    "    \"gender\": sorted(list(train_data[\"gender\"].unique())),\n",
    "    \"native_country\": sorted(list(train_data[\"native_country\"].unique())),\n",
    "}\n",
    "# Name of the column to be used as instances weight.\n",
    "WEIGHT_COLUMN_NAME = \"fnlwgt\"\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + [WEIGHT_COLUMN_NAME] else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"income_bracket\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\" <=50K\", \" >50K\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c61eb",
   "metadata": {},
   "source": [
    "# Configure the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22c4ef45",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_DECAY = 0.0001\n",
    "DROPOUT_RATE = 0.2\n",
    "BATCH_SIZE = 265\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "NUM_TRANSFORMER_BLOCKS = 3  # Number of transformer blocks.\n",
    "NUM_HEADS = 4  # Number of attention heads.\n",
    "EMBEDDING_DIMS = 16  # Embedding dimensions of the categorical features.\n",
    "MLP_HIDDEN_UNITS_FACTORS = [\n",
    "    2,\n",
    "    1,\n",
    "]  # MLP hidden layer units, as factors of the number of inputs.\n",
    "NUM_MLP_BLOCKS = 2  # Number of MLP blocks in the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d041a2c6",
   "metadata": {},
   "source": [
    "# Implement data reading pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de4c5209",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SOUFIANI Assia\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:2453: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "#define an input function that reads and parses the file, then converts features and labels into atf.data.Dataset for training or evaluation\n",
    "target_label_lookup = layers.StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_example(features, target):\n",
    "    target_index = target_label_lookup(target)\n",
    "    weights = features.pop(WEIGHT_COLUMN_NAME)\n",
    "    return features, target_index, weights\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, batch_size=128, shuffle=False):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        na_value=\"?\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(prepare_example, num_parallel_calls=tf.data.AUTOTUNE, deterministic=False)\n",
    "    return dataset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72ee8aa",
   "metadata": {},
   "source": [
    "# Implement a training and evaluation procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "215d2e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(\n",
    "    model,\n",
    "    train_data_file,\n",
    "    test_data_file,\n",
    "    num_epochs,\n",
    "    learning_rate,\n",
    "    weight_decay,\n",
    "    batch_size,\n",
    "):\n",
    "\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.BinaryCrossentropy(),\n",
    "        metrics=[keras.metrics.BinaryAccuracy(name=\"accuracy\")],\n",
    "    )\n",
    "\n",
    "    train_dataset = get_dataset_from_csv(train_data_file, batch_size, shuffle=True)\n",
    "    validation_dataset = get_dataset_from_csv(test_data_file, batch_size)\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    history = model.fit(\n",
    "        train_dataset, epochs=num_epochs, validation_data=validation_dataset\n",
    "    )\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    _, accuracy = model.evaluate(validation_dataset, verbose=0)\n",
    "\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18f2939",
   "metadata": {},
   "source": [
    "# Create model inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cc5e347",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the inputs for the models as a dictionary, where the key is the feature name, and the value is a keras.layers.Input tensor with the corresponding feature shape and data type\n",
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0995fb5",
   "metadata": {},
   "source": [
    "# Encode features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18e0b649",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(inputs, embedding_dims):\n",
    "\n",
    "    encoded_categorical_feature_list = []\n",
    "    numerical_feature_list = []\n",
    "\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "\n",
    "            # Get the vocabulary of the categorical feature.\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "\n",
    "            # Create a lookup to convert string values to an integer indices.\n",
    "            # Since we are not using a mask token nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and  num_oov_indices to 0.\n",
    "            lookup = layers.StringLookup(\n",
    "                vocabulary=vocabulary,\n",
    "                mask_token=None,\n",
    "                num_oov_indices=0,\n",
    "                output_mode=\"int\",\n",
    "            )\n",
    "\n",
    "            # Convert the string input values into integer indices.\n",
    "            encoded_feature = lookup(inputs[feature_name])\n",
    "\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=len(vocabulary), output_dim=embedding_dims\n",
    "            )\n",
    "\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_categorical_feature = embedding(encoded_feature)\n",
    "            encoded_categorical_feature_list.append(encoded_categorical_feature)\n",
    "\n",
    "        else:\n",
    "\n",
    "            # Use the numerical features as-is.\n",
    "            numerical_feature = tf.expand_dims(inputs[feature_name], -1)\n",
    "            numerical_feature_list.append(numerical_feature)\n",
    "\n",
    "    return encoded_categorical_feature_list, numerical_feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c40999",
   "metadata": {},
   "source": [
    "# Implement an MLP block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cab5f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(hidden_units, dropout_rate, activation, normalization_layer, name=None):\n",
    "\n",
    "    mlp_layers = []\n",
    "    for units in hidden_units:\n",
    "        mlp_layers.append(normalization_layer),\n",
    "        mlp_layers.append(layers.Dense(units, activation=activation))\n",
    "        mlp_layers.append(layers.Dropout(dropout_rate))\n",
    "\n",
    "    return keras.Sequential(mlp_layers, name=name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3297e8",
   "metadata": {},
   "source": [
    "# Experiment 1: a baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92f6f405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 109629\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "#In the first experiment, we create a simple multi-layer feed-forward network\n",
    "def create_baseline_model(\n",
    "    embedding_dims, num_mlp_blocks, mlp_hidden_units_factors, dropout_rate\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Concatenate all features.\n",
    "    features = layers.concatenate(\n",
    "        encoded_categorical_feature_list + numerical_feature_list\n",
    "    )\n",
    "    # Compute Feedforward layer units.\n",
    "    feedforward_units = [features.shape[-1]]\n",
    "\n",
    "    # Create several feedforwad layers with skip connections.\n",
    "    for layer_idx in range(num_mlp_blocks):\n",
    "        features = create_mlp(\n",
    "            hidden_units=feedforward_units,\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{layer_idx}\",\n",
    "        )(features)\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "baseline_model = create_baseline_model(\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    num_mlp_blocks=NUM_MLP_BLOCKS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", baseline_model.count_params())\n",
    "keras.utils.plot_model(baseline_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b073867c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/15\n",
      "    122/Unknown - 11s 25ms/step - loss: 112047.3203 - accuracy: 0.7416WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 14s 43ms/step - loss: 111984.3125 - accuracy: 0.7419 - val_loss: 95111.1641 - val_accuracy: 0.7826\n",
      "Epoch 2/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 91052.0625 - accuracy: 0.7677WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 28ms/step - loss: 90957.3984 - accuracy: 0.7678 - val_loss: 70683.0391 - val_accuracy: 0.8119\n",
      "Epoch 3/15\n",
      "121/123 [============================>.] - ETA: 0s - loss: 75625.3359 - accuracy: 0.7937WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 29ms/step - loss: 75709.8750 - accuracy: 0.7935 - val_loss: 70394.0234 - val_accuracy: 0.8013\n",
      "Epoch 4/15\n",
      "121/123 [============================>.] - ETA: 0s - loss: 72211.9844 - accuracy: 0.8031WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 29ms/step - loss: 72253.1172 - accuracy: 0.8029 - val_loss: 67116.4922 - val_accuracy: 0.8096\n",
      "Epoch 5/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 70690.7031 - accuracy: 0.8073WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 29ms/step - loss: 70687.8984 - accuracy: 0.8076 - val_loss: 72786.0859 - val_accuracy: 0.7869\n",
      "Epoch 6/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 69438.6250 - accuracy: 0.8101WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 28ms/step - loss: 69394.1953 - accuracy: 0.8104 - val_loss: 67808.7891 - val_accuracy: 0.8075\n",
      "Epoch 7/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 68507.2500 - accuracy: 0.8117WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 30ms/step - loss: 68507.2500 - accuracy: 0.8117 - val_loss: 67182.8984 - val_accuracy: 0.8113\n",
      "Epoch 8/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 68063.9531 - accuracy: 0.8133WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 30ms/step - loss: 68079.8047 - accuracy: 0.8135 - val_loss: 66605.6016 - val_accuracy: 0.8117\n",
      "Epoch 9/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 67813.9453 - accuracy: 0.8144WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 29ms/step - loss: 67780.2031 - accuracy: 0.8147 - val_loss: 66548.5781 - val_accuracy: 0.8153\n",
      "Epoch 10/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 67430.4062 - accuracy: 0.8154WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 29ms/step - loss: 67422.8438 - accuracy: 0.8155 - val_loss: 67368.7500 - val_accuracy: 0.8195\n",
      "Epoch 11/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 67273.2734 - accuracy: 0.8168WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 30ms/step - loss: 67273.2734 - accuracy: 0.8168 - val_loss: 67350.4219 - val_accuracy: 0.8168\n",
      "Epoch 12/15\n",
      "121/123 [============================>.] - ETA: 0s - loss: 67053.1250 - accuracy: 0.8155WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 29ms/step - loss: 67066.1406 - accuracy: 0.8155 - val_loss: 67217.4922 - val_accuracy: 0.8135\n",
      "Epoch 13/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 66833.7969 - accuracy: 0.8167WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 29ms/step - loss: 66833.7969 - accuracy: 0.8167 - val_loss: 66417.1641 - val_accuracy: 0.8147\n",
      "Epoch 14/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 66733.2031 - accuracy: 0.8195WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 4s 29ms/step - loss: 66733.2031 - accuracy: 0.8195 - val_loss: 66439.3672 - val_accuracy: 0.8173\n",
      "Epoch 15/15\n",
      "122/123 [============================>.] - ETA: 0s - loss: 65549.3359 - accuracy: 0.8221WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 3s 28ms/step - loss: 65551.6484 - accuracy: 0.8225 - val_loss: 69049.3516 - val_accuracy: 0.8205\n",
      "Model training finished\n",
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 82.05%\n"
     ]
    }
   ],
   "source": [
    "#train and evaluate the baseline model\n",
    "history = run_experiment(\n",
    "    model=baseline_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a4c8e",
   "metadata": {},
   "source": [
    "# Experiment 2: TabTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f03fd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total model weights: 87479\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "def create_tabtransformer_classifier(\n",
    "    num_transformer_blocks,\n",
    "    num_heads,\n",
    "    embedding_dims,\n",
    "    mlp_hidden_units_factors,\n",
    "    dropout_rate,\n",
    "    use_column_embedding=False,\n",
    "):\n",
    "\n",
    "    # Create model inputs.\n",
    "    inputs = create_model_inputs()\n",
    "    # encode features.\n",
    "    encoded_categorical_feature_list, numerical_feature_list = encode_inputs(\n",
    "        inputs, embedding_dims\n",
    "    )\n",
    "    # Stack categorical feature embeddings for the Tansformer.\n",
    "    encoded_categorical_features = tf.stack(encoded_categorical_feature_list, axis=1)\n",
    "    # Concatenate numerical features.\n",
    "    numerical_features = layers.concatenate(numerical_feature_list)\n",
    "\n",
    "    # Add column embedding to categorical feature embeddings.\n",
    "    if use_column_embedding:\n",
    "        num_columns = encoded_categorical_features.shape[1]\n",
    "        column_embedding = layers.Embedding(\n",
    "            input_dim=num_columns, output_dim=embedding_dims\n",
    "        )\n",
    "        column_indices = tf.range(start=0, limit=num_columns, delta=1)\n",
    "        encoded_categorical_features = encoded_categorical_features + column_embedding(\n",
    "            column_indices\n",
    "        )\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for block_idx in range(num_transformer_blocks):\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            key_dim=embedding_dims,\n",
    "            dropout=dropout_rate,\n",
    "            name=f\"multihead_attention_{block_idx}\",\n",
    "        )(encoded_categorical_features, encoded_categorical_features)\n",
    "        # Skip connection 1.\n",
    "        x = layers.Add(name=f\"skip_connection1_{block_idx}\")(\n",
    "            [attention_output, encoded_categorical_features]\n",
    "        )\n",
    "        # Layer normalization 1.\n",
    "        x = layers.LayerNormalization(name=f\"layer_norm1_{block_idx}\", epsilon=1e-6)(x)\n",
    "        # Feedforward.\n",
    "        feedforward_output = create_mlp(\n",
    "            hidden_units=[embedding_dims],\n",
    "            dropout_rate=dropout_rate,\n",
    "            activation=keras.activations.gelu,\n",
    "            normalization_layer=layers.LayerNormalization(epsilon=1e-6),\n",
    "            name=f\"feedforward_{block_idx}\",\n",
    "        )(x)\n",
    "        # Skip connection 2.\n",
    "        x = layers.Add(name=f\"skip_connection2_{block_idx}\")([feedforward_output, x])\n",
    "        # Layer normalization 2.\n",
    "        encoded_categorical_features = layers.LayerNormalization(\n",
    "            name=f\"layer_norm2_{block_idx}\", epsilon=1e-6\n",
    "        )(x)\n",
    "\n",
    "    # Flatten the \"contextualized\" embeddings of the categorical features.\n",
    "    categorical_features = layers.Flatten()(encoded_categorical_features)\n",
    "    # Apply layer normalization to the numerical features.\n",
    "    numerical_features = layers.LayerNormalization(epsilon=1e-6)(numerical_features)\n",
    "    # Prepare the input for the final MLP block.\n",
    "    features = layers.concatenate([categorical_features, numerical_features])\n",
    "\n",
    "    # Compute MLP hidden_units.\n",
    "    mlp_hidden_units = [\n",
    "        factor * features.shape[-1] for factor in mlp_hidden_units_factors\n",
    "    ]\n",
    "    # Create final MLP.\n",
    "    features = create_mlp(\n",
    "        hidden_units=mlp_hidden_units,\n",
    "        dropout_rate=dropout_rate,\n",
    "        activation=keras.activations.selu,\n",
    "        normalization_layer=layers.BatchNormalization(),\n",
    "        name=\"MLP\",\n",
    "    )(features)\n",
    "\n",
    "    # Add a sigmoid as a binary classifer.\n",
    "    outputs = layers.Dense(units=1, activation=\"sigmoid\", name=\"sigmoid\")(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tabtransformer_model = create_tabtransformer_classifier(\n",
    "    num_transformer_blocks=NUM_TRANSFORMER_BLOCKS,\n",
    "    num_heads=NUM_HEADS,\n",
    "    embedding_dims=EMBEDDING_DIMS,\n",
    "    mlp_hidden_units_factors=MLP_HIDDEN_UNITS_FACTORS,\n",
    "    dropout_rate=DROPOUT_RATE,\n",
    ")\n",
    "\n",
    "print(\"Total model weights:\", tabtransformer_model.count_params())\n",
    "keras.utils.plot_model(tabtransformer_model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bf0be6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n",
      "Epoch 1/15\n",
      "    123/Unknown - 26s 84ms/step - loss: 79678.6719 - accuracy: 0.7993WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 30s 117ms/step - loss: 79678.6719 - accuracy: 0.7993 - val_loss: 65838.6641 - val_accuracy: 0.8363\n",
      "Epoch 2/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 68231.6250 - accuracy: 0.8271WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 97ms/step - loss: 68231.6250 - accuracy: 0.8271 - val_loss: 64879.8711 - val_accuracy: 0.8390\n",
      "Epoch 3/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 66666.0938 - accuracy: 0.8300WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 7s 57ms/step - loss: 66666.0938 - accuracy: 0.8300 - val_loss: 63772.5078 - val_accuracy: 0.8423\n",
      "Epoch 4/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 64694.7109 - accuracy: 0.8362WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 10s 81ms/step - loss: 64694.7109 - accuracy: 0.8362 - val_loss: 62314.8945 - val_accuracy: 0.8445\n",
      "Epoch 5/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 64099.6875 - accuracy: 0.8382WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 10s 81ms/step - loss: 64099.6875 - accuracy: 0.8382 - val_loss: 62068.1641 - val_accuracy: 0.8436\n",
      "Epoch 6/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 63032.1367 - accuracy: 0.8410WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 99ms/step - loss: 63032.1367 - accuracy: 0.8410 - val_loss: 62788.7969 - val_accuracy: 0.8394\n",
      "Epoch 7/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 62799.9023 - accuracy: 0.8406WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 100ms/step - loss: 62799.9023 - accuracy: 0.8406 - val_loss: 61966.3516 - val_accuracy: 0.8421\n",
      "Epoch 8/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 62506.8945 - accuracy: 0.8414WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 13s 103ms/step - loss: 62506.8945 - accuracy: 0.8414 - val_loss: 62037.7852 - val_accuracy: 0.8412\n",
      "Epoch 9/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 62077.6680 - accuracy: 0.8423WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 97ms/step - loss: 62077.6680 - accuracy: 0.8423 - val_loss: 61652.0781 - val_accuracy: 0.8418\n",
      "Epoch 10/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 62036.7773 - accuracy: 0.8420WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 98ms/step - loss: 62036.7773 - accuracy: 0.8420 - val_loss: 61815.9570 - val_accuracy: 0.8421\n",
      "Epoch 11/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 61642.2773 - accuracy: 0.8431WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 97ms/step - loss: 61642.2773 - accuracy: 0.8431 - val_loss: 61326.9531 - val_accuracy: 0.8428\n",
      "Epoch 12/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 61610.9375 - accuracy: 0.8441WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 95ms/step - loss: 61610.9375 - accuracy: 0.8441 - val_loss: 64163.2656 - val_accuracy: 0.8342\n",
      "Epoch 13/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 61506.3594 - accuracy: 0.8435WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 98ms/step - loss: 61506.3594 - accuracy: 0.8435 - val_loss: 61624.1133 - val_accuracy: 0.8421\n",
      "Epoch 14/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 61355.1797 - accuracy: 0.8440WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 97ms/step - loss: 61355.1797 - accuracy: 0.8440 - val_loss: 61807.7891 - val_accuracy: 0.8427\n",
      "Epoch 15/15\n",
      "123/123 [==============================] - ETA: 0s - loss: 61408.8164 - accuracy: 0.8446WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n",
      "123/123 [==============================] - 12s 94ms/step - loss: 61408.8164 - accuracy: 0.8446 - val_loss: 61335.9453 - val_accuracy: 0.8442\n",
      "Model training finished\n",
      "WARNING:tensorflow:`evaluate()` received a value for `sample_weight`, but `weighted_metrics` were not provided.  Did you mean to pass metrics to `weighted_metrics` in `compile()`?  If this is intentional you can pass `weighted_metrics=[]` to `compile()` in order to silence this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 84.42%\n"
     ]
    }
   ],
   "source": [
    "#train and evaluate the TabTransformer model\n",
    "history = run_experiment(\n",
    "    model=tabtransformer_model,\n",
    "    train_data_file=train_data_file,\n",
    "    test_data_file=test_data_file,\n",
    "    num_epochs=NUM_EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2540f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa7426",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
